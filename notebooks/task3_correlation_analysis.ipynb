{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK 3: NEWS SENTIMENT & STOCK MOVEMENT CORRELATION ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/liluebuy/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import nltk\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import textblob, if not available we'll use VADER only\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    TEXTBLOB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TextBlob not available, using VADER only\")\n",
    "    TEXTBLOB_AVAILABLE = False\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "print(\"=== TASK 3: NEWS SENTIMENT & STOCK MOVEMENT CORRELATION ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading news data with proper date parsing...\n",
      "Most mentioned stock: MRK\n",
      "Downloading MRK data from 2019-06-12 to 2020-06-11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 252 trading days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_mixed_dates_fixed(date_series):\n",
    "    # Convert all to timezone-naive first\n",
    "    dates_parsed = pd.to_datetime(date_series, errors='coerce')\n",
    "    # Remove timezone info from all dates\n",
    "    dates_parsed = dates_parsed.dt.tz_localize(None)\n",
    "    return dates_parsed\n",
    "\n",
    "print(\"Loading news data with proper date parsing...\")\n",
    "df_news = pd.read_csv('../data/raw_analyst_ratings.csv')\n",
    "df_news['date'] = parse_mixed_dates_fixed(df_news['date'])\n",
    "\n",
    "# Get the most mentioned stock from news data\n",
    "top_stock = df_news['stock'].value_counts().head(1).index[0]\n",
    "print(f\"Most mentioned stock: {top_stock}\")\n",
    "\n",
    "# Download stock data for the same period as news\n",
    "end_date = df_news['date'].max()\n",
    "start_date = end_date - timedelta(days=365)  # 1 year of data\n",
    "\n",
    "# Convert to string for yfinance\n",
    "start_str = start_date.strftime('%Y-%m-%d')\n",
    "end_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Downloading {top_stock} data from {start_str} to {end_str}...\")\n",
    "stock_data = yf.download(top_stock, start=start_str, end=end_str)\n",
    "stock_data.columns = [col[0] for col in stock_data.columns]  # Flatten columns\n",
    "\n",
    "print(f\"Downloaded {len(stock_data)} trading days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATE ALIGNMENT ===\n",
      "\n",
      "News data before cleaning: 1407328\n",
      "News data after removing NaN dates: 55987\n",
      "News date range: 2011-04-27 to 2020-06-11\n",
      "Stock date range: 2019-06-12 to 2020-06-10\n",
      "News articles on trading days: 31846\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== DATE ALIGNMENT ===\\n\")\n",
    "\n",
    "# First, check for and remove any NaN dates\n",
    "print(f\"News data before cleaning: {len(df_news)}\")\n",
    "df_news_clean = df_news.dropna(subset=['date']).copy()\n",
    "print(f\"News data after removing NaN dates: {len(df_news_clean)}\")\n",
    "\n",
    "# Extract date-only for alignment\n",
    "df_news_clean['date_only'] = df_news_clean['date'].dt.date\n",
    "stock_data['date_only'] = stock_data.index.date\n",
    "\n",
    "print(f\"News date range: {df_news_clean['date_only'].min()} to {df_news_clean['date_only'].max()}\")\n",
    "print(f\"Stock date range: {stock_data['date_only'].min()} to {stock_data['date_only'].max()}\")\n",
    "\n",
    "# Filter news to match stock trading days\n",
    "news_in_range = df_news_clean[df_news_clean['date_only'].isin(stock_data['date_only'])]\n",
    "print(f\"News articles on trading days: {len(news_in_range)}\")\n",
    "\n",
    "# Use the cleaned data for the rest of the analysis\n",
    "df_news = df_news_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SENTIMENT ANALYSIS ===\n",
      "\n",
      "Analyzing sentiment for news headlines...\n",
      "TextBlob sentiment analysis completed\n",
      "Sentiment analysis completed!\n",
      "Sentiment statistics (VADER):\n",
      "  Mean: 0.0666\n",
      "  Std:  0.3135\n",
      "  Min:  -0.9382\n",
      "  Max:  0.9666\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SENTIMENT ANALYSIS ===\\n\")\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"Analyze sentiment using VADER\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0.0\n",
    "    scores = sia.polarity_scores(str(text))\n",
    "    return scores['compound']  # Returns between -1 (negative) and +1 (positive)\n",
    "\n",
    "# Apply sentiment analysis to headlines\n",
    "print(\"Analyzing sentiment for news headlines...\")\n",
    "df_news['sentiment_vader'] = df_news['headline'].apply(analyze_sentiment_vader)\n",
    "\n",
    "# If TextBlob is available, use it too\n",
    "if TEXTBLOB_AVAILABLE:\n",
    "    def analyze_sentiment_textblob(text):\n",
    "        if pd.isna(text):\n",
    "            return 0.0\n",
    "        blob = TextBlob(str(text))\n",
    "        return blob.sentiment.polarity  # Returns between -1 and +1\n",
    "    \n",
    "    df_news['sentiment_textblob'] = df_news['headline'].apply(analyze_sentiment_textblob)\n",
    "    print(\"TextBlob sentiment analysis completed\")\n",
    "else:\n",
    "    df_news['sentiment_textblob'] = df_news['sentiment_vader']\n",
    "    print(\"Using VADER sentiment for both measures\")\n",
    "\n",
    "print(\"Sentiment analysis completed!\")\n",
    "print(f\"Sentiment statistics (VADER):\")\n",
    "print(f\"  Mean: {df_news['sentiment_vader'].mean():.4f}\")\n",
    "print(f\"  Std:  {df_news['sentiment_vader'].std():.4f}\")\n",
    "print(f\"  Min:  {df_news['sentiment_vader'].min():.4f}\")\n",
    "print(f\"  Max:  {df_news['sentiment_vader'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STOCK RETURNS CALCULATION ===\n",
      "\n",
      "Stock returns calculated:\n",
      "            Close  daily_return_pct\n",
      "count  252.000000        251.000000\n",
      "mean    66.047177          0.021362\n",
      "std      3.398457          1.973164\n",
      "min     53.279583         -8.899009\n",
      "25%     64.597492         -0.827409\n",
      "50%     66.082150          0.000000\n",
      "75%     67.683992          0.942763\n",
      "max     73.248100          7.783653\n",
      "Trading days with returns: 251\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STOCK RETURNS CALCULATION ===\\n\")\n",
    "\n",
    "# Calculate daily returns\n",
    "stock_data['daily_return'] = stock_data['Close'].pct_change()\n",
    "stock_data['daily_return_pct'] = stock_data['daily_return'] * 100  # Percentage\n",
    "\n",
    "print(\"Stock returns calculated:\")\n",
    "print(stock_data[['Close', 'daily_return_pct']].describe())\n",
    "\n",
    "# Remove the first row (NaN return)\n",
    "stock_data_clean = stock_data.dropna(subset=['daily_return']).copy()\n",
    "print(f\"Trading days with returns: {len(stock_data_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AGGREGATE DAILY SENTIMENT ===\n",
      "\n",
      "Daily sentiment summary:\n",
      "Days with news: 2528\n",
      "Average articles per day: 22.15\n",
      "       sentiment_vader_mean  article_count  sentiment_textblob_mean\n",
      "count           2528.000000    2528.000000              2528.000000\n",
      "mean               0.076574      22.146756                 0.047167\n",
      "std                0.156459      68.144109                 0.110425\n",
      "min               -0.802000       1.000000                -1.000000\n",
      "25%                0.000000       3.000000                 0.000000\n",
      "50%                0.063450       9.000000                 0.025000\n",
      "75%                0.148425      17.000000                 0.075050\n",
      "max                0.790600     973.000000                 1.000000\n"
     ]
    }
   ],
   "source": [
    "#  Aggregate Daily Sentiment\n",
    "print(\"\\n=== AGGREGATE DAILY SENTIMENT ===\\n\")\n",
    "\n",
    "# Group news by date and calculate average daily sentiment\n",
    "daily_sentiment = df_news.groupby('date_only').agg({\n",
    "    'sentiment_vader': ['mean', 'count'],\n",
    "    'sentiment_textblob': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "daily_sentiment.columns = ['sentiment_vader_mean', 'article_count', 'sentiment_textblob_mean']\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "\n",
    "print(\"Daily sentiment summary:\")\n",
    "print(f\"Days with news: {len(daily_sentiment)}\")\n",
    "print(f\"Average articles per day: {daily_sentiment['article_count'].mean():.2f}\")\n",
    "print(daily_sentiment.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Merge Sentiment with Stock Returns\n",
    "print(\"\\n=== MERGING DATASETS ===\\n\")\n",
    "\n",
    "# Merge sentiment with stock returns\n",
    "merged_data = pd.merge(\n",
    "    daily_sentiment,\n",
    "    stock_data_clean[['date_only', 'daily_return', 'daily_return_pct', 'Close']],\n",
    "    on='date_only',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset size: {len(merged_data)}\")\n",
    "print(\"Merged data sample:\")\n",
    "print(merged_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MERGING DATASETS ===\n",
      "\n",
      "Merged dataset size: 251\n",
      "Merged data sample:\n",
      "    date_only  sentiment_vader_mean  article_count  sentiment_textblob_mean  \\\n",
      "0  2019-06-13                0.1178             19                   0.1382   \n",
      "1  2019-06-14                0.1229             27                   0.1623   \n",
      "2  2019-06-17                0.1595             13                   0.0881   \n",
      "3  2019-06-18                0.1441             18                   0.1132   \n",
      "4  2019-06-19                0.1417             19                   0.0887   \n",
      "\n",
      "   daily_return  daily_return_pct      Close  \n",
      "0     -0.009676         -0.967646  64.660477  \n",
      "1      0.005221          0.522125  64.998085  \n",
      "2      0.006282          0.628176  65.406387  \n",
      "3      0.014286          1.428575  66.340767  \n",
      "4      0.010297          1.029715  67.023888  \n"
     ]
    }
   ],
   "source": [
    "# Step 7: Merge Sentiment with Stock Returns (Redo)\n",
    "print(\"\\n=== MERGING DATASETS ===\\n\")\n",
    "\n",
    "# Group news by date and calculate average daily sentiment\n",
    "daily_sentiment = df_news.groupby('date_only').agg({\n",
    "    'sentiment_vader': ['mean', 'count'],\n",
    "    'sentiment_textblob': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "daily_sentiment.columns = ['sentiment_vader_mean', 'article_count', 'sentiment_textblob_mean']\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "\n",
    "# Merge sentiment with stock returns\n",
    "merged_data = pd.merge(\n",
    "    daily_sentiment,\n",
    "    stock_data_clean[['date_only', 'daily_return', 'daily_return_pct', 'Close']],\n",
    "    on='date_only',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset size: {len(merged_data)}\")\n",
    "print(\"Merged data sample:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CORRELATION ANALYSIS ===\n",
      "\n",
      "PEARSON CORRELATION RESULTS:\n",
      "VADER Sentiment vs Daily Returns: 0.1763\n",
      "TextBlob Sentiment vs Daily Returns: 0.0852\n",
      "\n",
      "Correlation Strength (VADER): Very Weak\n",
      "Correlation Strength (TextBlob): Very Weak\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Correlation Analysis\n",
    "print(\"\\n=== CORRELATION ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "corr_vader = pearsonr(merged_data['sentiment_vader_mean'], merged_data['daily_return'])[0]\n",
    "corr_textblob = pearsonr(merged_data['sentiment_textblob_mean'], merged_data['daily_return'])[0]\n",
    "\n",
    "print(\"PEARSON CORRELATION RESULTS:\")\n",
    "print(f\"VADER Sentiment vs Daily Returns: {corr_vader:.4f}\")\n",
    "print(f\"TextBlob Sentiment vs Daily Returns: {corr_textblob:.4f}\")\n",
    "\n",
    "# Interpret correlation strength\n",
    "def interpret_correlation(corr):\n",
    "    abs_corr = abs(corr)\n",
    "    if abs_corr >= 0.7:\n",
    "        return \"Strong\"\n",
    "    elif abs_corr >= 0.5:\n",
    "        return \"Moderate\" \n",
    "    elif abs_corr >= 0.3:\n",
    "        return \"Weak\"\n",
    "    else:\n",
    "        return \"Very Weak\"\n",
    "\n",
    "print(f\"\\nCorrelation Strength (VADER): {interpret_correlation(corr_vader)}\")\n",
    "print(f\"Correlation Strength (TextBlob): {interpret_correlation(corr_textblob)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
